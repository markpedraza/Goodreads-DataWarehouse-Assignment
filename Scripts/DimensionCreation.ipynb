{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a664ff65",
   "metadata": {},
   "source": [
    "# Dimension Creation Script\n",
    "### We will create the following dimensions/facts\n",
    " - dim_book\n",
    " - dim_review\n",
    " - dim_genre\n",
    " - dim_author\n",
    " - facts_interactions (AKA facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e4ec0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gcsfs in /opt/conda/miniconda3/lib/python3.11/site-packages (2023.12.2.post1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (3.10.10)\n",
      "Requirement already satisfied: decorator>4.1.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (5.1.1)\n",
      "Requirement already satisfied: fsspec==2023.12.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (2023.12.2)\n",
      "Requirement already satisfied: google-auth>=1.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (2.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (1.1.0)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (2.13.0)\n",
      "Requirement already satisfied: requests in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (2.31.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-auth>=1.2->gcsfs) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-auth>=1.2->gcsfs) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-cloud-storage->gcsfs) (2.21.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-cloud-storage->gcsfs) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-cloud-storage->gcsfs) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-cloud-storage->gcsfs) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->gcsfs) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->gcsfs) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->gcsfs) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->gcsfs) (2024.8.30)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.65.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (4.24.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.25.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-crc32c<2.0dev,>=1.0->google-cloud-storage->gcsfs) (1.16.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.2.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/miniconda3/lib/python3.11/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-cloud-storage->gcsfs) (2.21)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: openpyxl in /opt/conda/miniconda3/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/miniconda3/lib/python3.11/site-packages (from openpyxl) (2.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gcsfs\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf401a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gcsfs\n",
    "import calendar\n",
    "from google.cloud import storage\n",
    "from google.cloud.exceptions import NotFound\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, lower, instr, date_format, split, rand, regexp_replace, trim, max, expr, element_at\n",
    "from pyspark.sql.functions import monotonically_increasing_id # virtually the same as factorize() from pandas.\n",
    "from pyspark.sql import DataFrame as PySparkDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed0ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"goodreads_bucket\"\n",
    "PROJECT = 'cis4400-individual-project'\n",
    "FRACTION = 0.01\n",
    "SEED = 645"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754eb6e",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3fe7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will read from the following folders\n",
    "#\n",
    "# cleaned/genre\n",
    "# cleaned/authors\n",
    "# cleaned/books\n",
    "# cleaned/interactions\n",
    "# cleaned/reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bcce65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author_id: integer (nullable = true)\n",
      " |-- average_rating: float (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ratings_count: integer (nullable = true)\n",
      " |-- text_reviews_count: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "829529"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the GCS bucket\n",
    "folder_name = \"authors\"\n",
    "gcs_path = f\"gs://{bucket_name}/cleaned/{folder_name}/\"\n",
    "\n",
    "# Read the Parquet files\n",
    "authors_df = spark.read.parquet(gcs_path)\n",
    "\n",
    "# Show the DataFrame schema and data\n",
    "authors_df.printSchema()\n",
    "authors_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b66a5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- children: long (nullable = true)\n",
      " |-- comics_graphic: long (nullable = true)\n",
      " |-- fantasy_paranormal: long (nullable = true)\n",
      " |-- fiction: long (nullable = true)\n",
      " |-- history: long (nullable = true)\n",
      " |-- mystery: long (nullable = true)\n",
      " |-- non_fiction: long (nullable = true)\n",
      " |-- poetry: long (nullable = true)\n",
      " |-- romance: long (nullable = true)\n",
      " |-- young_adult: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2360655"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the GCS bucket\n",
    "folder_name = \"genre\"\n",
    "gcs_path = f\"gs://{bucket_name}/cleaned/{folder_name}/\"\n",
    "\n",
    "# Read the Parquet files\n",
    "genre_df = spark.read.parquet(gcs_path)\n",
    "\n",
    "# Show the DataFrame schema and data\n",
    "genre_df.printSchema()\n",
    "genre_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99b6055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- author_id: string (nullable = true)\n",
      " |    |    |-- role: string (nullable = true)\n",
      " |-- average_rating: float (nullable = true)\n",
      " |-- book_id: integer (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- edition_information: string (nullable = true)\n",
      " |-- format: string (nullable = true)\n",
      " |-- image_url: string (nullable = true)\n",
      " |-- is_ebook: integer (nullable = true)\n",
      " |-- isbn: integer (nullable = true)\n",
      " |-- isbn13: long (nullable = true)\n",
      " |-- kindle_asin: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- num_pages: integer (nullable = true)\n",
      " |-- popular_shelves: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- count: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- publication_day: integer (nullable = true)\n",
      " |-- publication_month: integer (nullable = true)\n",
      " |-- publication_year: integer (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- ratings_count: integer (nullable = true)\n",
      " |-- series: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- similar_books: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- text_reviews_count: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- title_without_series: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- work_id: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2360655"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the GCS bucket\n",
    "folder_name = \"books\"\n",
    "gcs_path = f\"gs://{bucket_name}/cleaned/{folder_name}/\"\n",
    "\n",
    "# Read the Parquet files\n",
    "books_df = spark.read.parquet(gcs_path)#.sample(withReplacement=False, fraction=FRACTION, seed=SEED)\n",
    "\n",
    "# Show the DataFrame schema and data\n",
    "books_df.printSchema()\n",
    "books_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a27f182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- is_read: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- is_reviewed: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91457444"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the GCS bucket\n",
    "folder_name = \"interactions\"\n",
    "gcs_path = f\"gs://{bucket_name}/cleaned/{folder_name}/\"\n",
    "\n",
    "# Read the Parquet files\n",
    "interactions_df = spark.read.parquet(gcs_path).sample(withReplacement=False, fraction=0.4, seed=SEED)\n",
    "\n",
    "# Show the DataFrame schema and data\n",
    "interactions_df.printSchema()\n",
    "interactions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ed82747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- book_id: integer (nullable = true)\n",
      " |-- n_comments: integer (nullable = true)\n",
      " |-- n_votes: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- date_added: timestamp (nullable = true)\n",
      " |-- date_updated: timestamp (nullable = true)\n",
      " |-- started_at: timestamp (nullable = true)\n",
      " |-- read_at: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15588978"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the GCS bucket\n",
    "folder_name = \"reviews\"\n",
    "gcs_path = f\"gs://{bucket_name}/cleaned/{folder_name}/\"\n",
    "\n",
    "# Read the Parquet files\n",
    "reviews_df = spark.read.parquet(gcs_path)#.sample(withReplacement=False, fraction=FRACTION, seed=SEED)\n",
    "\n",
    "# Show the DataFrame schema and data\n",
    "reviews_df.printSchema()\n",
    "reviews_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259ced9",
   "metadata": {},
   "source": [
    "# Function to help with dimension creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc69a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ids(df,id_column_name=None):\n",
    "    #Transforms PySpark dataframe\n",
    "    if isinstance(df, PySparkDataFrame):\n",
    "        print(\"Transforming PySpark DataFrame...\")\n",
    "        \n",
    "        if id_column_name is None:\n",
    "            id_column_name = \"id\"\n",
    "        \n",
    "        # Extract the original schema\n",
    "        original_schema = df.schema\n",
    "\n",
    "        # Add an ID column using zipWithIndex\n",
    "        df_with_index = (\n",
    "            df.rdd\n",
    "            .zipWithIndex()  # Add an index to each row\n",
    "            .map(lambda x: Row(**dict(x[0].asDict(), id=x[1])))  # Add 'id' to each row\n",
    "        )\n",
    "\n",
    "        # Define the new schema with the ID column added\n",
    "        new_schema = original_schema.add(id_column_name, \"long\")\n",
    "\n",
    "        # Create a new DataFrame with the updated schema\n",
    "        df_with_index = spark.createDataFrame(df_with_index, schema=new_schema)\n",
    "        \n",
    "        print(\"Completed transforming PySpark DataFrame.\")\n",
    "        return df_with_index\n",
    "    \n",
    "    #No valid dataframe found\n",
    "    else:\n",
    "        print(\"ERROR: INVALID DATAFRAME - NO PROCEDURE APPLIED.\")\n",
    "        print(\"Did you try inputting a PySpark Dataframe?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02f153e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check how unique a column is. This will be used to verify all ID's in an ID column are unique.\n",
    "def check_uniqueness(df, col_name):\n",
    "    if isinstance(df, PySparkDataFrame):\n",
    "        total_count = df.count()\n",
    "        distinct_count = df.select(col_name).distinct().count()\n",
    "    elif isinstance(df, pd.DataFrame):\n",
    "        total_count = len(df)\n",
    "        distinct_count = df[col_name].nunique()\n",
    "\n",
    "    if total_count == distinct_count:\n",
    "        print(f\"The column '{col_name}' contains all unique values.\")\n",
    "    else:\n",
    "        print(f\"The column '{col_name}' contains duplicates. Total rows: {total_count}, Distinct rows: {distinct_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8859fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize array. Append all dims and tables here. At the end, we loop through this list to save all dataframes.\n",
    "tables = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050fe90",
   "metadata": {},
   "source": [
    "# DIMENSION CREATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f213d4",
   "metadata": {},
   "source": [
    "# Dim Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3986464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming PySpark DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=====================================================>(166 + 1) / 167]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed transforming PySpark DataFrame.\n",
      "root\n",
      " |-- book_id: integer (nullable = true)\n",
      " |-- n_comments: integer (nullable = true)\n",
      " |-- n_votes: integer (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- date_added: timestamp (nullable = true)\n",
      " |-- date_updated: timestamp (nullable = true)\n",
      " |-- started_at: timestamp (nullable = true)\n",
      " |-- read_at: timestamp (nullable = true)\n",
      " |-- dim_review_id: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Create Dim\n",
    "dim_review = reviews_df.drop(\"rating\")\n",
    "\n",
    "#Remove duplicates\n",
    "dim_review = dim_review.distinct()\n",
    "\n",
    "#Add unique ID\n",
    "dim_review = create_ids(dim_review,\"dim_review_id\")\n",
    "\n",
    "#Add to tables\n",
    "tables.append([dim_review, \"dim_review\"])\n",
    "\n",
    "#Show\n",
    "dim_review.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844b2ca",
   "metadata": {},
   "source": [
    "# Dim Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bbcd33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming PySpark DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:============================>                            (6 + 6) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed transforming PySpark DataFrame.\n",
      "root\n",
      " |-- author_id: integer (nullable = true)\n",
      " |-- average_rating: float (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ratings_count: integer (nullable = true)\n",
      " |-- text_reviews_count: integer (nullable = true)\n",
      " |-- dim_author_id: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Create Dim\n",
    "dim_author = authors_df\n",
    "\n",
    "#Remove duplicates\n",
    "dim_author = dim_author.distinct()\n",
    "\n",
    "#Add unique ID\n",
    "dim_author = create_ids(dim_author,\"dim_author_id\")\n",
    "\n",
    "#Add to tables\n",
    "tables.append([dim_author, \"dim_author\"])\n",
    "\n",
    "#Show\n",
    "dim_author.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25c8ba",
   "metadata": {},
   "source": [
    "# Dim Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2800a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming PySpark DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:======================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed transforming PySpark DataFrame.\n",
      "root\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- children: long (nullable = true)\n",
      " |-- comics_graphic: long (nullable = true)\n",
      " |-- fantasy_paranormal: long (nullable = true)\n",
      " |-- fiction: long (nullable = true)\n",
      " |-- history: long (nullable = true)\n",
      " |-- mystery: long (nullable = true)\n",
      " |-- non_fiction: long (nullable = true)\n",
      " |-- poetry: long (nullable = true)\n",
      " |-- romance: long (nullable = true)\n",
      " |-- young_adult: long (nullable = true)\n",
      " |-- dim_genre_id: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Create Dim\n",
    "dim_genre = genre_df\n",
    "\n",
    "#Remove duplicates\n",
    "dim_genre = dim_genre.distinct()\n",
    "\n",
    "#Add unique ID\n",
    "dim_genre = create_ids(dim_genre,\"dim_genre_id\")\n",
    "\n",
    "#Add to tables\n",
    "tables.append([dim_genre, \"dim_genre\"])\n",
    "\n",
    "#Show\n",
    "dim_genre.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21080c2e",
   "metadata": {},
   "source": [
    "# Dim Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26bf6665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming PySpark DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:==================================================>     (26 + 3) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed transforming PySpark DataFrame.\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- average_rating: float (nullable = true)\n",
      " |-- book_id: integer (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- edition_information: string (nullable = true)\n",
      " |-- format: string (nullable = true)\n",
      " |-- image_url: string (nullable = true)\n",
      " |-- is_ebook: integer (nullable = true)\n",
      " |-- isbn: integer (nullable = true)\n",
      " |-- isbn13: long (nullable = true)\n",
      " |-- kindle_asin: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- num_pages: integer (nullable = true)\n",
      " |-- publication_day: integer (nullable = true)\n",
      " |-- publication_month: integer (nullable = true)\n",
      " |-- publication_year: integer (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- ratings_count: integer (nullable = true)\n",
      " |-- text_reviews_count: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- title_without_series: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- author_1_id: string (nullable = true)\n",
      " |-- author_2_id: string (nullable = true)\n",
      " |-- author_3_id: string (nullable = true)\n",
      " |-- dim_book_id: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 31:====================================================>   (27 + 2) / 29]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Create Dim\n",
    "dim_book = books_df\n",
    "\n",
    "#Drop unnecessary columns\n",
    "dim_book = dim_book.drop(\"similar_books\",\"popular_shelves\",\"series\",\"work_id\")\n",
    "\n",
    "#Remove duplicates\n",
    "dim_book = dim_book.distinct()\n",
    "\n",
    "# Flatten the array for \"authors\".\n",
    "# Although this may exclude any author past the 3rd one, we are doing simple data analysis.\n",
    "# We could also extract flatten for the roles too but I believe this would be too much for only specific analysis.\n",
    "dim_book = dim_book.withColumn(\"author_1_id\", element_at(expr(\"transform(authors, x -> x.author_id)\"), 1)) \\\n",
    "       .withColumn(\"author_2_id\", element_at(expr(\"transform(authors, x -> x.author_id)\"), 2)) \\\n",
    "       .withColumn(\"author_3_id\", element_at(expr(\"transform(authors, x -> x.author_id)\"), 3))\n",
    "dim_book = dim_book.drop(\"authors\")\n",
    "\n",
    "# Change author_id's to int\n",
    "dim_book = dim_book.withColumn(\"author_1_id\", col(\"author_1_id\").cast(\"int\"))\n",
    "dim_book = dim_book.withColumn(\"author_2_id\", col(\"author_2_id\").cast(\"int\"))\n",
    "dim_book = dim_book.withColumn(\"author_3_id\", col(\"author_3_id\").cast(\"int\"))\n",
    "\n",
    "# Transfer new author ids to facts table and remove them here. \n",
    "# To start, create a new df with only book_id and the author ids. We will join this on facts\n",
    "author_ids_df = dim_book.select(\"book_id\",\"author_1_id\",\"author_2_id\",\"author_3_id\")\n",
    "\n",
    "# Drop them in book dimension as they are no longer needed here. \n",
    "dim_book = dim_book.drop(\"author_1_id\",\"author_2_id\",\"author_3_id\")\n",
    "\n",
    "#Add unique ID\n",
    "dim_book = create_ids(dim_book,\"dim_book_id\")\n",
    "\n",
    "#Add to tables\n",
    "tables.append([dim_book, \"dim_book\"])\n",
    "\n",
    "#Show\n",
    "dim_book.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b754a",
   "metadata": {},
   "source": [
    "# Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f2bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- is_read: string (nullable = true)\n",
      " |-- is_reviewed: string (nullable = true)\n",
      " |-- fact_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create facts\n",
    "facts = interactions_df\n",
    "\n",
    "# Add foreign IDs, then drop foreign columns excluding its id.\n",
    "facts = facts.join(\n",
    "    reviews_df,\n",
    "    on=[\"user_id\", \"book_id\"], \n",
    "    how=\"left\" \n",
    ")\n",
    "\n",
    "# Join the new df we made with the author id's. \n",
    "facts = facts.join(\n",
    "    author_ids_df,\n",
    "    on=\"book_id\", \n",
    "    how=\"left\" \n",
    ")\n",
    "# Delete it as we dont need it anymore. \n",
    "del author_ids_df\n",
    "\n",
    "facts = facts.drop(\"date_added\", \"date_updated\", \"n_comments\",\n",
    "                  \"n_votes\", \"read_at\", \"review_text\",\n",
    "                  \"review_id\", \"review_text\", \"started_at\")\n",
    "\n",
    "\n",
    "#Remove duplicates\n",
    "facts = facts.distinct()\n",
    "\n",
    "# Add unique IDs\n",
    "facts = facts.withColumn(\"fact_id\", monotonically_increasing_id()+1)\n",
    "\n",
    "# Change book_id and rating to int\n",
    "facts = facts.withColumn(\"book_id\", col(\"book_id\").cast(\"int\"))\n",
    "facts = facts.withColumn(\"rating\", col(\"rating\").cast(\"int\"))\n",
    "\n",
    "# Add to tables\n",
    "tables.append([facts, \"facts\"])\n",
    "\n",
    "# Show\n",
    "facts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfcb301",
   "metadata": {},
   "source": [
    "# Functions for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a08dbf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder_exists(folder_name):\n",
    "    \n",
    "    # Grab the bucket\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    \n",
    "    # List objects in the bucket with the folder prefix\n",
    "    blobs = bucket.list_blobs(prefix=folder_name + '/')\n",
    "    \n",
    "    # Check if any blob exists under this folder\n",
    "    for blob in blobs:\n",
    "        if blob.name.startswith(folder_name + '/'):\n",
    "            print(f\"Folder '{folder_name}' exists in bucket '{bucket_name}'.\")\n",
    "            return True\n",
    "    \n",
    "    print(f\"Folder '{folder_name}' does not exist in bucket '{bucket_name}'.\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "795902f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(path, name):\n",
    "    \n",
    "    if check_folder_exists(name): \n",
    "        print(path+name+'/'+' already exists. Abandoning folder creation')\n",
    "        return\n",
    "    \n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # Define the folder path\n",
    "    folder_path = path+name+'/'\n",
    "\n",
    "    # Create a dummy file in the folder (this will simulate the folder in GCS)\n",
    "    # The 'blob' will not be a real file, but will create the \"folder\"\n",
    "    blob = bucket.blob(folder_path + 'placeholder.txt')  # You can name it anything, like 'placeholder.txt'\n",
    "\n",
    "    # Upload an empty string or any content to simulate the folder creation\n",
    "    blob.upload_from_string('')\n",
    "\n",
    "    print(f\"Folder '{folder_path}' created successfully in the bucket '{bucket_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10020e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files_named(name_of_file_to_delete):\n",
    "    # Initialize a client\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Specify the bucket name\n",
    "    _bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # List and delete all files named \"placeholder.txt\"\n",
    "    blobs = _bucket.list_blobs()\n",
    "\n",
    "    count_deleted = 0\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith(name_of_file_to_delete):\n",
    "            blob.delete()\n",
    "            print(f\"Deleted: {blob.name}\")\n",
    "            count_deleted += 1\n",
    "            \n",
    "    \n",
    "    print(f\"Deletion of all '{name_of_file_to_delete}' files is complete. Total deleted: \"+str(count_deleted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3cd12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df,name,save_as=\"parquet\",gcs_path=None):\n",
    "\n",
    "    #we must define the bucket variable at the start of the script, before calling this function.\n",
    "    \n",
    "    # Define the GCS path for saving the file\n",
    "    if gcs_path == None:\n",
    "        gcs_path = 'gs://'+bucket_name+'/dim_ready/'+name+'/'+name+'.'+save_as\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Creating '{name}' folder...\")\n",
    "    create_folder('dim_ready/',name)\n",
    "    delete_files_named(\"placeholder.txt\")\n",
    "    \n",
    "    print(\"Beginning saving process...\")\n",
    "    flag = 0 # this var will let us know is there as been an error\n",
    "    \n",
    "    #Saves Pandas dataframe\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        # Initialize GCS file system\n",
    "        fs = gcsfs.GCSFileSystem(project=PROJECT)\n",
    "\n",
    "        # Save DataFrame to the \"cleaned\" folder in the GCS bucket\n",
    "        with fs.open(gcs_path, 'w') as f:\n",
    "            if save_as == \"csv\":\n",
    "                df.to_csv(f, index=False)\n",
    "            else:\n",
    "                print(\"INVALD FILE FORMAT. TRY [csv]\")\n",
    "                flag+=1\n",
    "        \n",
    "        if flag == 0:\n",
    "            print(\"Successfully saved \"+name+ \"!\")\n",
    "    \n",
    "    #Saves PySpark dataframe\n",
    "    elif isinstance(df, PySparkDataFrame):\n",
    "        if save_as == \"parquet\":\n",
    "            df.write.parquet(gcs_path,mode=\"overwrite\")\n",
    "        elif save_as == \"csv\":\n",
    "            df.write.csv(gcs_path,mode=\"overwrite\")\n",
    "        elif save_as == \"avro\":\n",
    "            df.write.format(\"avro\").save(gcs_path,mode=\"overwrite\")\n",
    "        else:\n",
    "            print(\"INVALD FILE FORMAT. TRY [csv] or [parquet] or [avro]\")\n",
    "            flag+=1\n",
    "        \n",
    "        if flag == 0:\n",
    "            print(\"Successfully saved \"+name+ \"!\")\n",
    "        \n",
    "    #No valid dataframe found\n",
    "    else:\n",
    "        print(\"ERROR: INVALID DATAFRAME - NO PROCEDURE APPLIED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45011af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function accepts a list of tables, that each contain 2 elements, the table itself, and the name of the table.\n",
    "#Example, table[0] = [df_1,\"df_1_name\"], table[1][0] = df_2, table[3][1] = \"df_4_name\"\n",
    "def save_list_of_df(list_of_tables,pandas_save=\"csv\",pyspark_save=\"parquet\"):\n",
    "    for table in list_of_tables:\n",
    "        if isinstance(table[0], pd.DataFrame):\n",
    "            save_df(table[0],table[1],save_as=pandas_save)\n",
    "        elif isinstance(table[0], PySparkDataFrame):\n",
    "            save_df(table[0],table[1],save_as=pyspark_save)\n",
    "        else:\n",
    "            print(\"ERROR: INVALID LIST TO SAVE. TRY PASSING FORMAT - list = ([table_name, df])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f175203b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 'dim_review' folder...\n",
      "Folder 'dim_review' does not exist in bucket 'goodreads_bucket'.\n",
      "Folder 'dim_ready/dim_review/' created successfully in the bucket 'goodreads_bucket'\n",
      "Deleted: dim_ready/dim_review/placeholder.txt\n",
      "Deletion of all 'placeholder.txt' files is complete. Total deleted: 1\n",
      "Beginning saving process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved dim_review!\n",
      "Creating 'dim_author' folder...\n",
      "Folder 'dim_author' does not exist in bucket 'goodreads_bucket'.\n",
      "Folder 'dim_ready/dim_author/' created successfully in the bucket 'goodreads_bucket'\n",
      "Deleted: dim_ready/dim_author/placeholder.txt\n",
      "Deletion of all 'placeholder.txt' files is complete. Total deleted: 1\n",
      "Beginning saving process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved dim_author!\n",
      "Creating 'dim_genre' folder...\n",
      "Folder 'dim_genre' does not exist in bucket 'goodreads_bucket'.\n",
      "Folder 'dim_ready/dim_genre/' created successfully in the bucket 'goodreads_bucket'\n",
      "Deleted: dim_ready/dim_genre/placeholder.txt\n",
      "Deletion of all 'placeholder.txt' files is complete. Total deleted: 1\n",
      "Beginning saving process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved dim_genre!\n",
      "Creating 'dim_book' folder...\n",
      "Folder 'dim_book' does not exist in bucket 'goodreads_bucket'.\n",
      "Folder 'dim_ready/dim_book/' created successfully in the bucket 'goodreads_bucket'\n",
      "Deleted: dim_ready/dim_book/placeholder.txt\n",
      "Deletion of all 'placeholder.txt' files is complete. Total deleted: 1\n",
      "Beginning saving process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 10:29:32 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved dim_book!\n",
      "Creating 'facts' folder...\n",
      "Folder 'facts' does not exist in bucket 'goodreads_bucket'.\n",
      "Folder 'dim_ready/facts/' created successfully in the bucket 'goodreads_bucket'\n",
      "Deleted: dim_ready/facts/placeholder.txt\n",
      "Deletion of all 'placeholder.txt' files is complete. Total deleted: 1\n",
      "Beginning saving process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved facts!\n"
     ]
    }
   ],
   "source": [
    "#We will simply save all Pandas dataframes as CSV's and all PySpark ones as parquet\n",
    "\n",
    "ENABLE_SAVE = True #boolean to quickly turn on/off saving. Good to turn off saving when debugging.\n",
    "\n",
    "if ENABLE_SAVE:\n",
    "    save_list_of_df(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d81ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
