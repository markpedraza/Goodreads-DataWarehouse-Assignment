{"cells":[{"cell_type":"markdown","id":"99649da0","metadata":{},"source":["# Clean Data Script"]},{"cell_type":"code","execution_count":1,"id":"04f558f0","metadata":{},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-20241210d-highperformance-deletefast-m.us-central1-f.c.cis4400-individual-project.internal:42431\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f5c3ed80c90>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"markdown","id":"819cd45c","metadata":{},"source":["# Import necessary packages and specify bucket and folder path"]},{"cell_type":"code","execution_count":2,"id":"0177d15e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting handyspark\n","  Downloading handyspark-0.2.2a1-py2.py3-none-any.whl.metadata (1.2 kB)\n","Requirement already satisfied: pyspark in /usr/lib/spark/python (from handyspark) (3.5.1)\n","Requirement already satisfied: matplotlib in /opt/conda/miniconda3/lib/python3.11/site-packages (from handyspark) (3.8.4)\n","Requirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.11/site-packages (from handyspark) (1.26.4)\n","Requirement already satisfied: scipy in /opt/conda/miniconda3/lib/python3.11/site-packages (from handyspark) (1.11.4)\n","Requirement already satisfied: seaborn in /opt/conda/miniconda3/lib/python3.11/site-packages (from handyspark) (0.13.2)\n","Requirement already satisfied: pandas in /opt/conda/miniconda3/lib/python3.11/site-packages (from handyspark) (2.1.4)\n","Requirement already satisfied: scikit-learn in /opt/conda/miniconda3/lib/python3.11/site-packages (from handyspark) (1.3.2)\n","Collecting findspark (from handyspark)\n","  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n","Requirement already satisfied: pyarrow in /opt/conda/miniconda3/lib/python3.11/site-packages (from handyspark) (14.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from matplotlib->handyspark) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/miniconda3/lib/python3.11/site-packages (from matplotlib->handyspark) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from matplotlib->handyspark) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from matplotlib->handyspark) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from matplotlib->handyspark) (23.1)\n","Requirement already satisfied: pillow>=8 in /opt/conda/miniconda3/lib/python3.11/site-packages (from matplotlib->handyspark) (10.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from matplotlib->handyspark) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/miniconda3/lib/python3.11/site-packages (from matplotlib->handyspark) (2.9.0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pandas->handyspark) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pandas->handyspark) (2024.2)\n","Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pyspark->handyspark) (0.10.9.7)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from scikit-learn->handyspark) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from scikit-learn->handyspark) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->handyspark) (1.16.0)\n","Downloading handyspark-0.2.2a1-py2.py3-none-any.whl (39 kB)\n","Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n","Installing collected packages: findspark, handyspark\n","Successfully installed findspark-2.0.1 handyspark-0.2.2a1\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mRequirement already satisfied: gcsfs in /opt/conda/miniconda3/lib/python3.11/site-packages (2023.12.2.post1)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (3.10.10)\n","Requirement already satisfied: decorator>4.1.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (5.1.1)\n","Requirement already satisfied: fsspec==2023.12.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (2023.12.2)\n","Requirement already satisfied: google-auth>=1.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (2.35.0)\n","Requirement already satisfied: google-auth-oauthlib in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (1.1.0)\n","Requirement already satisfied: google-cloud-storage in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (2.13.0)\n","Requirement already satisfied: requests in /opt/conda/miniconda3/lib/python3.11/site-packages (from gcsfs) (2.31.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.16.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-auth>=1.2->gcsfs) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-auth>=1.2->gcsfs) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-auth>=1.2->gcsfs) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-cloud-storage->gcsfs) (2.21.0)\n","Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-cloud-storage->gcsfs) (2.4.1)\n","Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-cloud-storage->gcsfs) (2.7.2)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-cloud-storage->gcsfs) (1.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->gcsfs) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->gcsfs) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->gcsfs) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->gcsfs) (2024.8.30)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.65.0)\n","Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (4.24.4)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.25.0)\n","Requirement already satisfied: cffi>=1.0.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from google-crc32c<2.0dev,>=1.0->google-cloud-storage->gcsfs) (1.16.0)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n","Requirement already satisfied: propcache>=0.2.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.2.0)\n","Requirement already satisfied: pycparser in /opt/conda/miniconda3/lib/python3.11/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-cloud-storage->gcsfs) (2.21)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mCollecting openpyxl\n","  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n","Collecting et-xmlfile (from openpyxl)\n","  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n","Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m250.9/250.9 kB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n","Installing collected packages: et-xmlfile, openpyxl\n","Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install handyspark\n","!pip install gcsfs\n","!pip install openpyxl"]},{"cell_type":"code","execution_count":3,"id":"a21f6d43","metadata":{},"outputs":[],"source":["import numpy as np\n","import gcsfs\n","from google.cloud import storage\n","import os\n","from pyspark.sql.functions import to_date, rand, col, sum, when, to_timestamp, trim\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, LongType, TimestampType, ArrayType\n","np.bool = np.bool_"]},{"cell_type":"code","execution_count":4,"id":"1532a16e","metadata":{},"outputs":[],"source":["from handyspark import *"]},{"cell_type":"code","execution_count":5,"id":"f52a8ecb","metadata":{},"outputs":[],"source":["# GCS bucket and folder\n","bucket_name = \"goodreads_bucket\"\n","folder_path = \"landing/\"\n","save_path = \"cleaned/\"\n","SEED = 645\n","SAMPLE_FRAC=0.01"]},{"cell_type":"markdown","id":"8323aff7","metadata":{},"source":["# Create schemas before reading the data"]},{"cell_type":"code","execution_count":6,"id":"e4bf6964","metadata":{},"outputs":[],"source":["# Reviews schema\n","\n","reviews_schema = StructType([\n","    StructField(\"book_id\", StringType(), nullable=True),\n","    StructField(\"date_added\", StringType(), nullable=True),\n","    StructField(\"date_updated\", StringType(), nullable=True),\n","    StructField(\"n_comments\", IntegerType(), nullable=True),\n","    StructField(\"n_votes\", IntegerType(), nullable=True),\n","    StructField(\"rating\", IntegerType(), nullable=True),\n","    StructField(\"read_at\", StringType(), nullable=True),\n","    StructField(\"review_id\", StringType(), nullable=True),\n","    StructField(\"review_text\", StringType(), nullable=True),\n","    StructField(\"started_at\", StringType(), nullable=True),\n","    StructField(\"user_id\", StringType(), nullable=True)\n","])"]},{"cell_type":"code","execution_count":7,"id":"59bdec07","metadata":{},"outputs":[],"source":["# Books schema\n","\n","# Define the schema for the nested structure\n","authors_schema = StructType([\n","    StructField(\"author_id\", StringType(), nullable=True),\n","    StructField(\"role\", StringType(), nullable=True)\n","])\n","\n","popular_shelves_schema = StructType([\n","    StructField(\"count\", StringType(), nullable=True),\n","    StructField(\"name\", StringType(), nullable=True)\n","])\n","\n","# Main schema\n","books_schema = StructType([\n","    StructField(\"asin\", StringType(), nullable=True),\n","    StructField(\"authors\", ArrayType(authors_schema), nullable=True),\n","    StructField(\"average_rating\", StringType(), nullable=True),\n","    StructField(\"book_id\", StringType(), nullable=True),\n","    StructField(\"country_code\", StringType(), nullable=True),\n","    StructField(\"description\", StringType(), nullable=True),\n","    StructField(\"edition_information\", StringType(), nullable=True),\n","    StructField(\"format\", StringType(), nullable=True),\n","    StructField(\"image_url\", StringType(), nullable=True),\n","    StructField(\"is_ebook\", IntegerType(), nullable=True),\n","    StructField(\"isbn\", StringType(), nullable=True),\n","    StructField(\"isbn13\", StringType(), nullable=True),\n","    StructField(\"kindle_asin\", StringType(), nullable=True),\n","    StructField(\"language_code\", StringType(), nullable=True),\n","    StructField(\"link\", StringType(), nullable=True),\n","    StructField(\"num_pages\", StringType(), nullable=True),\n","    StructField(\"popular_shelves\", ArrayType(popular_shelves_schema), nullable=True),\n","    StructField(\"publication_day\", StringType(), nullable=True),\n","    StructField(\"publication_month\", StringType(), nullable=True),\n","    StructField(\"publication_year\", StringType(), nullable=True),\n","    StructField(\"publisher\", StringType(), nullable=True),\n","    StructField(\"ratings_count\", StringType(), nullable=True),\n","    StructField(\"series\", ArrayType(StringType(), containsNull=True), nullable=True),\n","    StructField(\"similar_books\", ArrayType(StringType(), containsNull=True), nullable=True),\n","    StructField(\"text_reviews_count\", StringType(), nullable=True),\n","    StructField(\"title\", StringType(), nullable=True),\n","    StructField(\"title_without_series\", StringType(), nullable=True),\n","    StructField(\"url\", StringType(), nullable=True),\n","    StructField(\"work_id\", StringType(), nullable=True)\n","])"]},{"cell_type":"code","execution_count":8,"id":"67924964","metadata":{},"outputs":[],"source":["# Interactions schema\n","\n","interactions_schema = StructType([\n","    StructField(\"user_id\", StringType(), nullable=True),\n","    StructField(\"book_id\", IntegerType(), nullable=True),\n","    StructField(\"is_read\", IntegerType(), nullable=True),\n","    StructField(\"rating\", IntegerType(), nullable=True),\n","    StructField(\"is_reviewed\", IntegerType(), nullable=True)\n","])"]},{"cell_type":"code","execution_count":9,"id":"d9a50cea","metadata":{},"outputs":[],"source":["# Author schema\n","\n","author_schema = StructType([\n","    StructField(\"author_id\", StringType(), nullable=True), \n","    StructField(\"average_rating\", StringType(), nullable=True), \n","    StructField(\"name\", StringType(), nullable=True), \n","    StructField(\"ratings_count\", StringType(), nullable=True),\n","    StructField(\"text_reviews_count\", StringType(), nullable=True)\n","])"]},{"cell_type":"code","execution_count":10,"id":"0bb95320","metadata":{},"outputs":[],"source":["# Genre schema\n","\n","genre_schema = StructType([\n","    StructField(\"book_id\", StringType(), True),  # Book ID, nullable\n","    StructField(\"genres\", StructType([  # Nested struct for genres\n","        StructField(\"children\", LongType(), True),\n","        StructField(\"comics, graphic\", LongType(), True),\n","        StructField(\"fantasy, paranormal\", LongType(), True),\n","        StructField(\"fiction\", LongType(), True),\n","        StructField(\"history, historical fiction, biography\", LongType(), True),\n","        StructField(\"mystery, thriller, crime\", LongType(), True),\n","        StructField(\"non-fiction\", LongType(), True),\n","        StructField(\"poetry\", LongType(), True),\n","        StructField(\"romance\", LongType(), True),\n","        StructField(\"young-adult\", LongType(), True)\n","    ]), True)  # Allow the genres struct to be nullable\n","])\n"]},{"cell_type":"markdown","id":"5988acfc","metadata":{},"source":["# Read all our data into spark dataframes\n","#### Files we want to get into datframes from the landing folder of our bucket\n"," - goodreads_book_authors.json\n"," - goodreads_book_genres_initial.json\n"," - goodreads_reviews_dedup.json\n"," - goodreads_books.json\n"," - goodreads_interactions.csv"]},{"cell_type":"code","execution_count":11,"id":"ffbfee7e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---------+--------------+-------------------+-------------+------------------+\n","|author_id|average_rating|               name|ratings_count|text_reviews_count|\n","+---------+--------------+-------------------+-------------+------------------+\n","|   604031|          3.98|   Ronald J. Fields|           49|                 7|\n","|   626222|          4.08|      Anita Diamant|       546796|             28716|\n","|    10333|          3.92|     Barbara Hambly|       122118|              5075|\n","|     9212|          3.68|    Jennifer Weiner|       888522|             36262|\n","|   149918|          3.82|      Nigel Pennick|         1740|                96|\n","|  3041852|          3.89|   Alfred J. Church|          947|                85|\n","|   215594|          4.17| Michael Halberstam|           23|                 6|\n","|    19158|          4.18|     Rachel Roberts|        13677|               486|\n","|  5807700|          3.99|         V.L. Locey|         3130|               986|\n","|  2983296|          3.48|Anton Szandor LaVey|        12628|               824|\n","+---------+--------------+-------------------+-------------+------------------+\n","only showing top 10 rows\n","\n","root\n"," |-- author_id: string (nullable = true)\n"," |-- average_rating: string (nullable = true)\n"," |-- name: string (nullable = true)\n"," |-- ratings_count: string (nullable = true)\n"," |-- text_reviews_count: string (nullable = true)\n","\n"]}],"source":["# goodreads_book_authors.json\n","\n","file_name = \"goodreads_book_authors.json\"\n","\n","# Path to the JSON files in the bucket\n","gcs_path = f\"gs://{bucket_name}/{folder_path}/{file_name}\"\n","\n","# Read JSON files from GCS\n","authors_df = spark.read.schema(author_schema).json(gcs_path)#.sample(withReplacement=False, fraction=SAMPLE_FRAC, seed=SEED)\n","\n","# Replace empty strings with nulls in all columns\n","authors_df = authors_df.replace(\"\", None)\n","\n","# Show the data\n","authors_df.show(10)\n","\n","# Print schema of the JSON\n","authors_df.printSchema()"]},{"cell_type":"code","execution_count":12,"id":"c560d89c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+--------------------+\n","| book_id|              genres|\n","+--------+--------------------+\n","| 5333265|{NULL, NULL, NULL...|\n","| 1333909|{NULL, NULL, NULL...|\n","| 7327624|{NULL, NULL, 31, ...|\n","| 6066819|{NULL, NULL, NULL...|\n","|  287140|{NULL, NULL, NULL...|\n","|  287141|{6, NULL, 1, 1, 9...|\n","|  378460|{NULL, NULL, NULL...|\n","| 6066812|{16, NULL, 32, 7,...|\n","|34883016|{NULL, NULL, NULL...|\n","|  287149|{NULL, NULL, NULL...|\n","+--------+--------------------+\n","only showing top 10 rows\n","\n","root\n"," |-- book_id: string (nullable = true)\n"," |-- genres: struct (nullable = true)\n"," |    |-- children: long (nullable = true)\n"," |    |-- comics, graphic: long (nullable = true)\n"," |    |-- fantasy, paranormal: long (nullable = true)\n"," |    |-- fiction: long (nullable = true)\n"," |    |-- history, historical fiction, biography: long (nullable = true)\n"," |    |-- mystery, thriller, crime: long (nullable = true)\n"," |    |-- non-fiction: long (nullable = true)\n"," |    |-- poetry: long (nullable = true)\n"," |    |-- romance: long (nullable = true)\n"," |    |-- young-adult: long (nullable = true)\n","\n"]}],"source":["# goodreads_book_genres_initial.json\n","\n","file_name = \"goodreads_book_genres_initial.json\"\n","\n","# Path to the JSON files in the bucket\n","gcs_path = f\"gs://{bucket_name}/{folder_path}/{file_name}\"\n","\n","# Read JSON files from GCS\n","genre_df = spark.read.schema(genre_schema).json(gcs_path)#.sample(withReplacement=False, fraction=SAMPLE_FRAC, seed=SEED)\n","\n","# Replace empty strings with nulls in all columns\n","genre_df = genre_df.replace(\"\", None)\n","\n","# Show the data\n","genre_df.show(10)\n","\n","# Print schema of the JSON\n","genre_df.printSchema()"]},{"cell_type":"code","execution_count":13,"id":"9011e932","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+--------------------+--------------------+----------+-------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","| book_id|          date_added|        date_updated|n_comments|n_votes|rating|             read_at|           review_id|         review_text|          started_at|             user_id|\n","+--------+--------------------+--------------------+----------+-------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|24375664|Fri Aug 25 13:55:...|Mon Oct 09 08:55:...|         0|     16|     5|Sat Oct 07 00:00:...|5cd416f3efc3f944f...|Mind blowingly co...|Sat Aug 26 00:00:...|8842281e1d1347389...|\n","|18245960|Sun Jul 30 07:44:...|Wed Aug 30 00:00:...|         1|     28|     5|Sat Aug 26 12:05:...|dfdbb7b0eb5a7e4c2...|This is a special...|Tue Aug 15 13:23:...|8842281e1d1347389...|\n","| 6392944|Mon Jul 24 02:48:...|Sun Jul 30 09:28:...|         0|      6|     3|Tue Jul 25 00:00:...|5e212a62bced17b4d...|I haven't read a ...|Mon Jul 24 00:00:...|8842281e1d1347389...|\n","|22078596|Mon Jul 24 02:33:...|Sun Jul 30 10:23:...|         4|     22|     4|Sun Jul 30 15:42:...|fdd13cad0695656be...|Fun, fast paced, ...|Tue Jul 25 00:00:...|8842281e1d1347389...|\n","| 6644782|Mon Jul 24 02:28:...|Thu Aug 24 00:07:...|         0|      8|     4|Sat Aug 05 00:00:...|bd0df91c9d918c0e4...|A fun book that g...|Sun Jul 30 00:00:...|8842281e1d1347389...|\n","| 1995421|Mon Mar 06 07:14:...|Mon Mar 06 07:15:...|         0|      4|     0|                NULL|7350a30a2f5c785b1...|Kevin highly reco...|                NULL|8842281e1d1347389...|\n","| 9460786|Tue Feb 28 17:52:...|Fri May 26 12:04:...|         3|     13|     4|Fri May 26 15:17:...|fc24c814e34ec6da3...|Giving a high rat...|Fri May 05 00:00:...|8842281e1d1347389...|\n","|29983426|Sun Jan 08 15:51:...|Tue Mar 07 01:09:...|         6|     26|     5|Mon Feb 20 00:00:...|18479ea761f5f7d66...|I decided to give...|Fri Jan 13 00:00:...|8842281e1d1347389...|\n","|29893493|Thu Dec 15 10:51:...|Sun Mar 12 23:33:...|         8|     29|     5|Thu Mar 09 15:34:...|c23406fb584d6304d...|I haven't read a ...|Tue Feb 28 17:55:...|8842281e1d1347389...|\n","|28114110|Sun Dec 11 16:18:...|Tue Dec 27 07:29:...|         0|     14|     4|Sat Dec 17 00:00:...|fb5bdf2100832503e...|Kevin Kelly, who ...|Sun Dec 11 00:00:...|8842281e1d1347389...|\n","+--------+--------------------+--------------------+----------+-------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","only showing top 10 rows\n","\n","root\n"," |-- book_id: string (nullable = true)\n"," |-- date_added: string (nullable = true)\n"," |-- date_updated: string (nullable = true)\n"," |-- n_comments: integer (nullable = true)\n"," |-- n_votes: integer (nullable = true)\n"," |-- rating: integer (nullable = true)\n"," |-- read_at: string (nullable = true)\n"," |-- review_id: string (nullable = true)\n"," |-- review_text: string (nullable = true)\n"," |-- started_at: string (nullable = true)\n"," |-- user_id: string (nullable = true)\n","\n"]}],"source":["# goodreads_reviews_dedup.json\n","\n","file_name = \"goodreads_reviews_dedup.json\"\n","\n","# Path to the JSON files in the bucket\n","gcs_path = f\"gs://{bucket_name}/{folder_path}/{file_name}\"\n","\n","# Read JSON files from GCS\n","reviews_df = spark.read.schema(reviews_schema).json(gcs_path)#.sample(withReplacement=False, fraction=SAMPLE_FRAC, seed=SEED)\n","\n","# Replace empty strings with nulls in all columns\n","reviews_df = reviews_df.replace(\"\", None)\n","\n","# Show the data\n","reviews_df.show(10)\n","\n","# Print schema of the JSON\n","reviews_df.printSchema()"]},{"cell_type":"code","execution_count":14,"id":"37dc8c99","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- asin: string (nullable = true)\n"," |-- authors: array (nullable = true)\n"," |    |-- element: struct (containsNull = true)\n"," |    |    |-- author_id: string (nullable = true)\n"," |    |    |-- role: string (nullable = true)\n"," |-- average_rating: string (nullable = true)\n"," |-- book_id: string (nullable = true)\n"," |-- country_code: string (nullable = true)\n"," |-- description: string (nullable = true)\n"," |-- edition_information: string (nullable = true)\n"," |-- format: string (nullable = true)\n"," |-- image_url: string (nullable = true)\n"," |-- is_ebook: integer (nullable = true)\n"," |-- isbn: string (nullable = true)\n"," |-- isbn13: string (nullable = true)\n"," |-- kindle_asin: string (nullable = true)\n"," |-- language_code: string (nullable = true)\n"," |-- link: string (nullable = true)\n"," |-- num_pages: string (nullable = true)\n"," |-- popular_shelves: array (nullable = true)\n"," |    |-- element: struct (containsNull = true)\n"," |    |    |-- count: string (nullable = true)\n"," |    |    |-- name: string (nullable = true)\n"," |-- publication_day: string (nullable = true)\n"," |-- publication_month: string (nullable = true)\n"," |-- publication_year: string (nullable = true)\n"," |-- publisher: string (nullable = true)\n"," |-- ratings_count: string (nullable = true)\n"," |-- series: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- similar_books: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- text_reviews_count: string (nullable = true)\n"," |-- title: string (nullable = true)\n"," |-- title_without_series: string (nullable = true)\n"," |-- url: string (nullable = true)\n"," |-- work_id: string (nullable = true)\n","\n"]}],"source":["# goodreads_books.json\n","\n","file_name = \"goodreads_books.json\"\n","\n","# Path to the JSON files in the bucket\n","gcs_path = f\"gs://{bucket_name}/{folder_path}/{file_name}\"\n","\n","# Read JSON files from GCS\n","books_df = spark.read.schema(books_schema).json(gcs_path)#.sample(withReplacement=False, fraction=SAMPLE_FRAC, seed=SEED)\n","\n","# Replace empty strings with nulls in all columns\n","books_df = books_df.replace(\"\", None)\n","\n","# Print schema of the JSON\n","books_df.printSchema()"]},{"cell_type":"code","execution_count":15,"id":"6838167d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 4:=====================================================>   (31 + 2) / 33]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+-------+-------+------+-----------+\n","|user_id|book_id|is_read|rating|is_reviewed|\n","+-------+-------+-------+------+-----------+\n","| 437231| 487488|      0|     0|          0|\n","| 516185|    670|      1|     5|          0|\n","| 153514|    577|      1|     5|          0|\n","|  54780| 151816|      0|     0|          0|\n","| 389284| 529441|      0|     0|          0|\n","| 356244|  15268|      1|     5|          0|\n","| 153342|   2766|      0|     0|          0|\n","| 361960| 201881|      0|     0|          0|\n","|  81753| 113006|      1|     3|          0|\n","| 132349| 633124|      1|     4|          0|\n","+-------+-------+-------+------+-----------+\n","\n","root\n"," |-- user_id: string (nullable = true)\n"," |-- book_id: string (nullable = true)\n"," |-- is_read: string (nullable = true)\n"," |-- rating: string (nullable = true)\n"," |-- is_reviewed: string (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# goodreads_interactions.csv\n","\n","file_name = \"goodreads_interactions.csv\"\n","\n","# Path to the JSON files in the bucket\n","gcs_path = f\"gs://{bucket_name}/{folder_path}/{file_name}\"\n","\n","# Read CSV file from GCS with header option\n","interactions_df = spark.read.option(\"header\", \"true\").csv(gcs_path)#.sample(withReplacement=False, fraction=SAMPLE_FRAC, seed=SEED)\n","\n","# Replace empty strings with nulls in all columns\n","interactions_df = interactions_df.replace(\"\", None)\n","\n","# Show the data\n","interactions_df.orderBy(rand()).limit(10).show(10)\n","\n","# Print schema\n","interactions_df.printSchema()"]},{"cell_type":"markdown","id":"c4aba349","metadata":{},"source":["# Functions to assist in cleaning process"]},{"cell_type":"code","execution_count":16,"id":"aca68e6e","metadata":{},"outputs":[],"source":["#function takes in a spark dataframe, then drops nulls in ALL rows. Accepts a list of columns to skip in this process\n","#This function is meant for dfs with a large amount of columns with small amount of nulls in each column.\n","#\n","#BEFORE USING THIS FUNCTION, DROP COLUMNS THAT HAVE A HIGH PERCENTAGE OF NULLS\n","#OTHERWISE THE WHOLE DF WILL PRACTICALLY BE DROPPED.\n","\n","def drop_nulls_in_rows(df, list_of_columns_to_ignore=None):\n","    #get a extremely small sample just so we can find the rows we are dealing with using pandas\n","    sample_sdf = df.limit(1)\n","    pandas_df = sample_sdf.toPandas()\n"," \n","    #Now we get our list of columns that we will drop nulls in\n","    columns = pandas_df.columns\n","    \n","    #If any value of the list_of_columns_to_ignore is in the columns, we take that column out.\n","    if list_of_columns_to_ignore is not None:\n","        # Filter out the columns that are to be ignored\n","        columns = [col for col in pandas_df.columns if col not in list_of_columns_to_ignore]\n","    \n","    \n","    #With the columns extracted, loop through every column and filter it so we dont get nulls.\n","    for column in columns:\n","        print(\"Removing nulls in \"+column+\"...\")\n","        df = df.filter(column+\" is not NULL\")\n","    \n","    return df"]},{"cell_type":"code","execution_count":17,"id":"a5f77e6d","metadata":{},"outputs":[],"source":["#function that drops a list of columns in a pyspark df\n","def drop_columns(df, list_of_columns):\n","    for column in list_of_columns:\n","        print(\"Removing column \"+column+\"...\")\n","        df = df.drop(column)\n","    \n","    return df"]},{"cell_type":"markdown","id":"a0f9aa56","metadata":{},"source":["# Checking NULLS"]},{"cell_type":"code","execution_count":18,"id":"1663c73f","metadata":{},"outputs":[],"source":["# Small function we will use to count nulls\n","def count_nulls(sdf):\n","    hsdf = HandyFrame(sdf)\n","    print(hsdf.isnull())"]},{"cell_type":"code","execution_count":19,"id":"f3950337","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/spark/python/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n","  warnings.warn(\n","/usr/lib/spark/python/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n","  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n","[Stage 5:======================================================>(124 + 1) / 125]\r"]},{"name":"stdout","output_type":"stream","text":["book_id               0\n","date_added            0\n","date_updated          0\n","n_comments            0\n","n_votes               0\n","rating                0\n","read_at         2766813\n","review_id             0\n","review_text        6938\n","started_at      6712475\n","user_id               0\n","Name: missing, dtype: int64\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Count nulls in each row, so we can eventually drop them\n","count_nulls(reviews_df)"]},{"cell_type":"code","execution_count":20,"id":"2fba2547","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 8:======================================================>(124 + 1) / 125]\r"]},{"name":"stdout","output_type":"stream","text":["Count of reviews_df: 15739967\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Count rows \n","reviews_df_count = reviews_df.count()\n","print(\"Count of reviews_df: \"+str(reviews_df_count))"]},{"cell_type":"code","execution_count":21,"id":"274df763","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/spark/python/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n","  warnings.warn(\n","/usr/lib/spark/python/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n","  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n","24/12/10 09:54:23 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","[Stage 11:======================================================> (67 + 2) / 69]\r"]},{"name":"stdout","output_type":"stream","text":["asin                    1891138\n","authors                       0\n","average_rating              524\n","book_id                       0\n","country_code                490\n","description              412233\n","edition_information     2142642\n","format                   646754\n","image_url                   490\n","is_ebook                2360655\n","isbn                     983373\n","isbn13                   780263\n","kindle_asin             1345725\n","language_code           1060153\n","link                        524\n","num_pages                764133\n","popular_shelves               0\n","publication_day         1024429\n","publication_month        882945\n","publication_year         599625\n","publisher                654362\n","ratings_count               524\n","series                        0\n","similar_books                 0\n","text_reviews_count          524\n","title                         7\n","title_without_series          7\n","url                         524\n","work_id                     524\n","Name: missing, dtype: int64\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Count nulls in each row, so we can eventually drop them\n","count_nulls(books_df)\n","\n","# Columns that must be transformed\n","#\n","# book_id"]},{"cell_type":"code","execution_count":22,"id":"34bc6ba1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 14:=======================================================>(68 + 1) / 69]\r"]},{"name":"stdout","output_type":"stream","text":["Count of books_df: 2360655\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Count rows \n","books_df_count = books_df.count()\n","print(\"Count of books_df: \"+str(books_df_count))"]},{"cell_type":"code","execution_count":23,"id":"0716e149","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/spark/python/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n","  warnings.warn(\n","/usr/lib/spark/python/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n","  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n","[Stage 17:======================================================> (32 + 1) / 33]\r"]},{"name":"stdout","output_type":"stream","text":["user_id        0\n","book_id        0\n","is_read        0\n","rating         0\n","is_reviewed    0\n","Name: missing, dtype: int64\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Count nulls in each row, so we can eventually drop them\n","count_nulls(interactions_df)\n","\n","# Columns that must be transformed\n","#\n","# None!"]},{"cell_type":"code","execution_count":24,"id":"dc4e0ac1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 20:====================================================>   (31 + 2) / 33]\r"]},{"name":"stdout","output_type":"stream","text":["Count of interactions_df: 228648342\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Count rows \n","interactions_df_count = interactions_df.count()\n","print(\"Count of interactions_df: \"+str(interactions_df_count))"]},{"cell_type":"code","execution_count":25,"id":"f68ae7bb","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/spark/python/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n","  warnings.warn(\n","/usr/lib/spark/python/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n","  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"]},{"name":"stdout","output_type":"stream","text":["author_id             0\n","average_rating        0\n","name                  5\n","ratings_count         0\n","text_reviews_count    0\n","Name: missing, dtype: int64\n"]},{"name":"stderr","output_type":"stream","text":["\r","[Stage 23:============================>                            (6 + 6) / 12]\r","\r","                                                                                \r"]}],"source":["count_nulls(authors_df)"]},{"cell_type":"code","execution_count":26,"id":"c02f6b76","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Count of authors_df: 829529\n"]},{"name":"stderr","output_type":"stream","text":["\r","[Stage 26:===================================================>    (11 + 1) / 12]\r","\r","                                                                                \r"]}],"source":["# Count rows \n","authors_df_count = authors_df.count()\n","print(\"Count of authors_df: \"+str(authors_df_count))"]},{"cell_type":"code","execution_count":27,"id":"a7c2a9f4","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/spark/python/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n","  warnings.warn(\n","/usr/lib/spark/python/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n","  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n","[Stage 29:======================================>                  (8 + 4) / 12]\r"]},{"name":"stdout","output_type":"stream","text":["book_id    0\n","genres     0\n","Name: missing, dtype: int64\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["count_nulls(genre_df)"]},{"cell_type":"code","execution_count":28,"id":"b48ef4a0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\r","[Stage 32:>                                                       (0 + 12) / 12]\r"]},{"name":"stdout","output_type":"stream","text":["Count of genre_df: 2360655\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Count rows \n","genre_df_count = genre_df.count()\n","print(\"Count of genre_df: \"+str(genre_df_count))"]},{"cell_type":"markdown","id":"b5757772","metadata":{},"source":["# Reformatting"]},{"cell_type":"code","execution_count":29,"id":"94494c03","metadata":{},"outputs":[],"source":["# Changes a list of columns to a certain datatype. This is for PySpark dataframes.\n","# You give it... \n","#  1. The dataframe you want to modify\n","#  2. The list of strings for the name of columns you want to be modified\n","#  3. The datetype you want to modify them to\n","\n","def change_cols_to_type(df, list_of_columns, data_type):\n","    for column in list_of_columns:\n","        df = df.withColumn(column, df[column].cast(data_type))\n","    return df"]},{"cell_type":"code","execution_count":30,"id":"2fb2328e","metadata":{"scrolled":true},"outputs":[],"source":["# For books_df, change all these columns below to their respective datatypes.\n","#\n","# average_rating : float         \n","# book_id : integer\n","# is_ebook : integer\n","# isbin : integer\n","# isbin13 : integer\n","# num_pages : integer\n","# publication_day : integer\n","# publication_month : integer\n","# publication_year : integer\n","# ratings_count : integer\n","# text_reviews_count : integer\n","\n","# For reviews_df, change all these columns below to their respective datatypes.\n","#\n","# book_id : integer"]},{"cell_type":"code","execution_count":31,"id":"f3d264df","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/spark/python/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n","  warnings.warn(\n","/usr/lib/spark/python/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n","  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["asin                    1891138\n","authors                       0\n","average_rating              524\n","book_id                       0\n","country_code                490\n","description              412233\n","edition_information     2142642\n","format                   646754\n","image_url                   490\n","is_ebook                2360655\n","isbn                    1307129\n","isbn13                   783792\n","kindle_asin             1345725\n","language_code           1060153\n","link                        524\n","num_pages                764133\n","popular_shelves               0\n","publication_day         1024429\n","publication_month        882945\n","publication_year         599625\n","publisher                654362\n","ratings_count               524\n","series                        0\n","similar_books                 0\n","text_reviews_count          524\n","title                         7\n","title_without_series          7\n","url                         524\n","work_id                     524\n","Name: missing, dtype: int64\n","+----------+-------------+--------+\n","|isbn      |isbn13       |is_ebook|\n","+----------+-------------+--------+\n","|312853122 |9780312853129|NULL    |\n","|743509986 |9780743509985|NULL    |\n","|NULL      |NULL         |NULL    |\n","|743294297 |9780743294294|NULL    |\n","|850308712 |9780850308716|NULL    |\n","|1599150603|9781599150604|NULL    |\n","|425040887 |9780425040881|NULL    |\n","|1934876569|9781934876565|NULL    |\n","|NULL      |9781370889471|NULL    |\n","|922915113 |9780922915118|NULL    |\n","+----------+-------------+--------+\n","\n"]}],"source":["#Change book cols to a more appropriate datatype\n","\n","#Columns to change\n","books_cols_to_int = [\"average_rating\", \"book_id\", \"isbn\", \"num_pages\", \"publication_day\",\n","                    \"publication_month\",\"publication_year\",\"ratings_count\",\"text_reviews_count\"]\n","books_cols_to_long = [\"isbn13\"]\n","books_cols_to_float = [\"average_rating\"]\n","\n","#Change columns\n","books_df = change_cols_to_type(books_df, books_cols_to_int, \"integer\")\n","books_df = change_cols_to_type(books_df, books_cols_to_long, \"long\")\n","books_df = change_cols_to_type(books_df, books_cols_to_float, \"float\")\n","\n","#Count nulls to make sure nothing wrong happened\n","count_nulls(books_df)\n","\n","#Show certain book columns after changing their datatypes\n","books_df.select(\"isbn\",\"isbn13\", \"is_ebook\").limit(10).show(truncate=False)"]},{"cell_type":"code","execution_count":32,"id":"6b7ef91a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/spark/python/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n","  warnings.warn(\n","/usr/lib/spark/python/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n","  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n","[Stage 39:=====================================================>(123 + 2) / 125]\r"]},{"name":"stdout","output_type":"stream","text":["book_id               0\n","date_added            0\n","date_updated          0\n","n_comments            0\n","n_votes               0\n","rating                0\n","read_at         2766813\n","review_id             0\n","review_text        6938\n","started_at      6712475\n","user_id               0\n","Name: missing, dtype: int64\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["#Change review cols to a more appropriate datatype\n","\n","#Columns to change\n","reviews_cols_to_int = [\"book_id\"]\n","\n","#Change columns\n","reviews_df = change_cols_to_type(reviews_df, reviews_cols_to_int, \"integer\")\n","\n","#Count nulls to make sure nothing wrong happened\n","count_nulls(reviews_df)"]},{"cell_type":"code","execution_count":33,"id":"df8af94c","metadata":{},"outputs":[],"source":["# int float string int int\n","# |-- author_id: string (nullable = true)\n","# |-- average_rating: string (nullable = true)\n","# |-- name: string (nullable = true)\n","# |-- ratings_count: string (nullable = true)\n","# |-- text_reviews_count: string (nullable = true)\n","\n","auth_cols_int = [\"author_id\", \"ratings_count\", \"text_reviews_count\"]\n","auth_cols_float = [\"average_rating\"]\n","\n","authors_df = change_cols_to_type(authors_df, auth_cols_int, \"integer\")\n","authors_df = change_cols_to_type(authors_df, auth_cols_float, \"float\")"]},{"cell_type":"code","execution_count":34,"id":"eb4da1dd","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+--------+--------------+------------------+-------+-------+-------+-----------+------+-------+-----------+\n","| book_id|children|comics_graphic|fantasy_paranormal|fiction|history|mystery|non_fiction|poetry|romance|young_adult|\n","+--------+--------+--------------+------------------+-------+-------+-------+-----------+------+-------+-----------+\n","| 5333265|       0|             0|                 0|      0|      1|      0|          0|     0|      0|          0|\n","| 1333909|       0|             0|                 0|    219|      5|      0|          0|     0|      0|          0|\n","| 7327624|       0|             0|                31|      8|      0|      1|          0|     1|      0|          0|\n","| 6066819|       0|             0|                 0|    555|      0|     10|          0|     0|     23|          0|\n","|  287140|       0|             0|                 0|      0|      0|      0|          3|     0|      0|          0|\n","|  287141|       6|             0|                 1|      1|      9|      0|          0|     0|      0|          1|\n","|  378460|       0|             0|                 0|      2|      0|      0|          0|     0|      0|          0|\n","| 6066812|      16|             0|                32|      7|      0|      0|          0|     0|      0|          8|\n","|34883016|       0|             0|                 0|      0|      0|      0|          0|     0|      3|          0|\n","|  287149|       0|             0|                 0|      0|      1|      0|         24|     0|      0|          0|\n","| 6066814|       0|             0|                 0|     19|     38|     38|          0|     0|      0|          0|\n","|33394837|       0|             0|                11|      3|      4|     12|          0|     0|      0|          0|\n","|   89371|       3|             0|                 0|     33|      0|      0|        266|     3|      0|          3|\n","|28575155|       0|             0|                 0|      0|      0|      0|          0|     0|      0|          0|\n","|   89373|       0|             0|                 0|   1760|     12|     31|          0|     0|      0|          0|\n","|   89375|       0|             6|                 0|     16|    178|      0|        534|     0|      0|          0|\n","|   89376|       0|             0|                 0|      0|      0|      0|        163|     0|      0|          0|\n","|   89377|     190|             0|                 0|    425|    330|      0|          0|     0|      0|         93|\n","|   89378|     109|             0|                 0|     13|      0|      0|          2|     0|      0|          1|\n","|21401188|       0|             0|                 1|      7|      0|      0|          0|     0|      1|          3|\n","+--------+--------+--------------+------------------+-------+-------+-------+-----------+------+-------+-----------+\n","only showing top 20 rows\n","\n"]}],"source":["# Flattening the genres dataframe\n","flat = genre_df.select(\n","    col(\"book_id\"),\n","    col(\"genres.children\").alias(\"children\"),\n","    col(\"genres.comics, graphic\").alias(\"comics_graphic\"),\n","    col(\"genres.fantasy, paranormal\").alias(\"fantasy_paranormal\"),\n","    col(\"genres.fiction\").alias(\"fiction\"),\n","    col(\"genres.history, historical fiction, biography\").alias(\"history\"),\n","    col(\"genres.mystery, thriller, crime\").alias(\"mystery\"),\n","    col(\"genres.non-fiction\").alias(\"non_fiction\"),\n","    col(\"genres.poetry\").alias(\"poetry\"),\n","    col(\"genres.romance\").alias(\"romance\"),\n","    col(\"genres.young-adult\").alias(\"young_adult\")\n",").fillna(0)\n","flat.show()\n","genre_df = flat\n","del flat"]},{"cell_type":"markdown","id":"ea06502f","metadata":{},"source":["# Begin cleaning process"]},{"cell_type":"markdown","id":"b764f9a4","metadata":{},"source":["# Change/Create columns to datetime"]},{"cell_type":"code","execution_count":35,"id":"322d4734","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------+------------------------------+------------------------------+------------------------------+\n","|date_added                    |date_updated                  |started_at                    |read_at                       |\n","+------------------------------+------------------------------+------------------------------+------------------------------+\n","|Fri Aug 25 13:55:02 -0700 2017|Mon Oct 09 08:55:59 -0700 2017|Sat Aug 26 00:00:00 -0700 2017|Sat Oct 07 00:00:00 -0700 2017|\n","|Sun Jul 30 07:44:10 -0700 2017|Wed Aug 30 00:00:26 -0700 2017|Tue Aug 15 13:23:18 -0700 2017|Sat Aug 26 12:05:52 -0700 2017|\n","|Mon Jul 24 02:48:17 -0700 2017|Sun Jul 30 09:28:03 -0700 2017|Mon Jul 24 00:00:00 -0700 2017|Tue Jul 25 00:00:00 -0700 2017|\n","|Mon Jul 24 02:33:09 -0700 2017|Sun Jul 30 10:23:54 -0700 2017|Tue Jul 25 00:00:00 -0700 2017|Sun Jul 30 15:42:05 -0700 2017|\n","|Mon Jul 24 02:28:14 -0700 2017|Thu Aug 24 00:07:20 -0700 2017|Sun Jul 30 00:00:00 -0700 2017|Sat Aug 05 00:00:00 -0700 2017|\n","|Mon Mar 06 07:14:44 -0800 2017|Mon Mar 06 07:15:21 -0800 2017|NULL                          |NULL                          |\n","|Tue Feb 28 17:52:08 -0800 2017|Fri May 26 12:04:23 -0700 2017|Fri May 05 00:00:00 -0700 2017|Fri May 26 15:17:45 -0700 2017|\n","|Sun Jan 08 15:51:29 -0800 2017|Tue Mar 07 01:09:18 -0800 2017|Fri Jan 13 00:00:00 -0800 2017|Mon Feb 20 00:00:00 -0800 2017|\n","|Thu Dec 15 10:51:26 -0800 2016|Sun Mar 12 23:33:51 -0700 2017|Tue Feb 28 17:55:35 -0800 2017|Thu Mar 09 15:34:06 -0800 2017|\n","|Sun Dec 11 16:18:32 -0800 2016|Tue Dec 27 07:29:33 -0800 2016|Sun Dec 11 00:00:00 -0800 2016|Sat Dec 17 00:00:00 -0800 2016|\n","+------------------------------+------------------------------+------------------------------+------------------------------+\n","\n"]}],"source":["spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n","\n","# Define the format of the date strings\n","date_format = \"EEE MMM dd HH:mm:ss Z yyyy\"\n","\n","# Apply transformations directly to `reviews_df`\n","reviews_df = reviews_df \\\n","    .withColumn(\"date_added_fix\", to_timestamp(col(\"date_added\"), date_format)) \\\n","    .withColumn(\"date_updated_fix\", to_timestamp(col(\"date_updated\"), date_format)) \\\n","    .withColumn(\"started_at_fix\", to_timestamp(col(\"started_at\"), date_format)) \\\n","    .withColumn(\"read_at_fix\", to_timestamp(col(\"read_at\"), date_format))\n","\n","# Show old columns\n","reviews_df.select(\"date_added\", \"date_updated\", \"started_at\", \"read_at\").limit(10).show(truncate=False)"]},{"cell_type":"code","execution_count":36,"id":"bebe17fe","metadata":{},"outputs":[],"source":["reviews_df = reviews_df.drop(\"date_added\", \"date_updated\", \"started_at\", \"read_at\")\n","reviews_df = reviews_df.withColumnRenamed(\"date_added_fix\", \"date_added\")\n","reviews_df = reviews_df.withColumnRenamed(\"date_updated_fix\", \"date_updated\")\n","reviews_df = reviews_df.withColumnRenamed(\"started_at_fix\", \"started_at\")\n","reviews_df = reviews_df.withColumnRenamed(\"read_at_fix\", \"read_at\")"]},{"cell_type":"code","execution_count":37,"id":"e4761436","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+-------------------+-------------------+-------------------+\n","|date_added         |date_updated       |started_at         |read_at            |\n","+-------------------+-------------------+-------------------+-------------------+\n","|2017-08-25 20:55:02|2017-10-09 15:55:59|2017-08-26 07:00:00|2017-10-07 07:00:00|\n","|2017-07-30 14:44:10|2017-08-30 07:00:26|2017-08-15 20:23:18|2017-08-26 19:05:52|\n","|2017-07-24 09:48:17|2017-07-30 16:28:03|2017-07-24 07:00:00|2017-07-25 07:00:00|\n","|2017-07-24 09:33:09|2017-07-30 17:23:54|2017-07-25 07:00:00|2017-07-30 22:42:05|\n","|2017-07-24 09:28:14|2017-08-24 07:07:20|2017-07-30 07:00:00|2017-08-05 07:00:00|\n","|2017-03-06 15:14:44|2017-03-06 15:15:21|NULL               |NULL               |\n","|2017-03-01 01:52:08|2017-05-26 19:04:23|2017-05-05 07:00:00|2017-05-26 22:17:45|\n","|2017-01-08 23:51:29|2017-03-07 09:09:18|2017-01-13 08:00:00|2017-02-20 08:00:00|\n","|2016-12-15 18:51:26|2017-03-13 06:33:51|2017-03-01 01:55:35|2017-03-09 23:34:06|\n","|2016-12-12 00:18:32|2016-12-27 15:29:33|2016-12-11 08:00:00|2016-12-17 08:00:00|\n","+-------------------+-------------------+-------------------+-------------------+\n","\n"]}],"source":["# Show the updated columns\n","reviews_df.select(\"date_added\", \"date_updated\", \"started_at\", \"read_at\").limit(10).show(truncate=False)"]},{"cell_type":"markdown","id":"d50d3674","metadata":{},"source":["# Save Data"]},{"cell_type":"code","execution_count":38,"id":"d5542c50","metadata":{},"outputs":[],"source":["def write_parquet_to_gcs(df, folder_name):\n","    \n","    client = storage.Client()\n","\n","    # Specify the path to the \"cleaned\" folder in the bucket\n","    _folder_path = f\"{save_path}{folder_name}\"\n","\n","    # Get the bucket and check if the folder already exists\n","    bucket = client.get_bucket(bucket_name)\n","    blobs = bucket.list_blobs(prefix=_folder_path)\n","\n","    # Check if folder exists by listing the files\n","    folder_exists = any(blob.name.startswith(_folder_path) for blob in blobs)\n","\n","    if not folder_exists:\n","        print(f\"Folder {_folder_path} does not exist. Creating and writing Parquet.\")\n","    else:\n","        print(f\"Folder {_folder_path} already exists. Overwriting content.\")\n","\n","    # Define the GCS path where the Parquet files will be written\n","    gcs_path = f\"gs://{bucket_name}/{_folder_path}/\"\n","\n","    # Write the DataFrame to Parquet in the specified folder\n","    df.write.parquet(gcs_path, mode='overwrite')  # Use 'overwrite' to write or replace existing content\n","\n","    print(f\"Data written to Parquet in folder: {_folder_path}\")"]},{"cell_type":"code","execution_count":42,"id":"bd2e012e","metadata":{},"outputs":[],"source":["#This is for the next few cells. If this option is True, then we will enable saving.\n","#This allows for when we 'reset and run all' and want the option to not save when debugging.\n","ENABLE_SAVE = True"]},{"cell_type":"code","execution_count":47,"id":"d4b33755","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Folder cleaned/reviews already exists. Overwriting content.\n"]},{"name":"stderr","output_type":"stream","text":["24/12/10 10:16:20 WARN TaskSetManager: Lost task 123.0 in stage 52.0 (TID 1382) (cluster-20241210d-highperformance-deletefast-w-0.us-central1-f.c.cis4400-individual-project.internal executor 20): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to gs://goodreads_bucket/cleaned/reviews.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:493)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n","writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n","into Parquet INT96 files can be dangerous, as the files may be read by Spark 2.x\n","or legacy versions of Hive later, which uses a legacy hybrid calendar that\n","is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n","details in SPARK-31404. You can set \"spark.sql.parquet.int96RebaseModeInWrite\" to \"LEGACY\" to rebase the\n","datetime values w.r.t. the calendar difference during writing, to get maximum\n","interoperability. Or set the config to \"CORRECTED\" to write the datetime\n","values as it is, if you are sure that the written files will only be read by\n","Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n","\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n","\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createTimestampRebaseFuncInWrite$1(DataSourceUtils.scala:232)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:211)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:210)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n","\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n","\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n","\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:476)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1399)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:483)\n","\t... 17 more\n","\n","24/12/10 10:16:25 ERROR TaskSetManager: Task 123 in stage 52.0 failed 4 times; aborting job\n","24/12/10 10:16:25 ERROR FileFormatWriter: Aborting job f03e431a-a3bb-4277-8d2e-734583c11fab.\n","org.apache.spark.SparkException: Job aborted due to stage failure: Task 123 in stage 52.0 failed 4 times, most recent failure: Lost task 123.3 in stage 52.0 (TID 1386) (cluster-20241210d-highperformance-deletefast-w-2.us-central1-f.c.cis4400-individual-project.internal executor 19): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to gs://goodreads_bucket/cleaned/reviews.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:493)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n","writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n","into Parquet INT96 files can be dangerous, as the files may be read by Spark 2.x\n","or legacy versions of Hive later, which uses a legacy hybrid calendar that\n","is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n","details in SPARK-31404. You can set \"spark.sql.parquet.int96RebaseModeInWrite\" to \"LEGACY\" to rebase the\n","datetime values w.r.t. the calendar difference during writing, to get maximum\n","interoperability. Or set the config to \"CORRECTED\" to write the datetime\n","values as it is, if you are sure that the written files will only be read by\n","Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n","\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n","\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createTimestampRebaseFuncInWrite$1(DataSourceUtils.scala:232)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:211)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:210)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n","\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n","\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n","\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:476)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1399)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:483)\n","\t... 17 more\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2457) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:380) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:329) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:377) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:197) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:473) ~[spark-catalyst_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [spark-sql-api_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:473) [spark-catalyst_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [spark-catalyst_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [spark-catalyst_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:449) [spark-catalyst_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859) [spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388) [spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361) [spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240) [spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792) [spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]\n","\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n","\tat java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) [py4j-0.10.9.7.jar:?]\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) [py4j-0.10.9.7.jar:?]\n","\tat py4j.Gateway.invoke(Gateway.java:282) [py4j-0.10.9.7.jar:?]\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) [py4j-0.10.9.7.jar:?]\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79) [py4j-0.10.9.7.jar:?]\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) [py4j-0.10.9.7.jar:?]\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106) [py4j-0.10.9.7.jar:?]\n","\tat java.base/java.lang.Thread.run(Thread.java:829) [?:?]\n","Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to gs://goodreads_bucket/cleaned/reviews.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774) ~[spark-catalyst_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:493) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\n","\t... 1 more\n","Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\n","writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n","into Parquet INT96 files can be dangerous, as the files may be read by Spark 2.x\n","or legacy versions of Hive later, which uses a legacy hybrid calendar that\n","is different from Spark 3.0+'s Proleptic Gregorian calendar. See more\n","details in SPARK-31404. You can set \"spark.sql.parquet.int96RebaseModeInWrite\" to \"LEGACY\" to rebase the\n","datetime values w.r.t. the calendar difference during writing, to get maximum\n","interoperability. Or set the config to \"CORRECTED\" to write the datetime\n","values as it is, if you are sure that the written files will only be read by\n","Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759) ~[spark-catalyst_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createTimestampRebaseFuncInWrite$1(DataSourceUtils.scala:232) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:211) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:210) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138) ~[parquet-hadoop-1.13.1-dataproc-1.0.0.jar:1.13.1-dataproc-1.0.0]\n","\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181) ~[parquet-hadoop-1.13.1-dataproc-1.0.0.jar:1.13.1-dataproc-1.0.0]\n","\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43) ~[parquet-hadoop-1.13.1-dataproc-1.0.0.jar:1.13.1-dataproc-1.0.0]\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:476) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1399) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:483) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\n","\t... 1 more\n"]},{"ename":"Py4JJavaError","evalue":"An error occurred while calling o925.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 123 in stage 52.0 failed 4 times, most recent failure: Lost task 123.3 in stage 52.0 (TID 1386) (cluster-20241210d-highperformance-deletefast-w-2.us-central1-f.c.cis4400-individual-project.internal executor 19): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to gs://goodreads_bucket/cleaned/reviews.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:493)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\nwriting dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\ninto Parquet INT96 files can be dangerous, as the files may be read by Spark 2.x\nor legacy versions of Hive later, which uses a legacy hybrid calendar that\nis different from Spark 3.0+'s Proleptic Gregorian calendar. See more\ndetails in SPARK-31404. You can set \"spark.sql.parquet.int96RebaseModeInWrite\" to \"LEGACY\" to rebase the\ndatetime values w.r.t. the calendar difference during writing, to get maximum\ninteroperability. Or set the config to \"CORRECTED\" to write the datetime\nvalues as it is, if you are sure that the written files will only be read by\nSpark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createTimestampRebaseFuncInWrite$1(DataSourceUtils.scala:232)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:211)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:210)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:476)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1399)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:483)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2457)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:380)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:329)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:377)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:197)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:473)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:473)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:449)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to gs://goodreads_bucket/cleaned/reviews.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:493)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\nwriting dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\ninto Parquet INT96 files can be dangerous, as the files may be read by Spark 2.x\nor legacy versions of Hive later, which uses a legacy hybrid calendar that\nis different from Spark 3.0+'s Proleptic Gregorian calendar. See more\ndetails in SPARK-31404. You can set \"spark.sql.parquet.int96RebaseModeInWrite\" to \"LEGACY\" to rebase the\ndatetime values w.r.t. the calendar difference during writing, to get maximum\ninteroperability. Or set the config to \"CORRECTED\" to write the datetime\nvalues as it is, if you are sure that the written files will only be read by\nSpark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createTimestampRebaseFuncInWrite$1(DataSourceUtils.scala:232)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:211)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:210)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:476)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1399)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:483)\n\t... 17 more\n","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","Cell \u001B[0;32mIn[47], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ENABLE_SAVE:\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mwrite_parquet_to_gcs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreviews_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreviews\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n","Cell \u001B[0;32mIn[38], line 24\u001B[0m, in \u001B[0;36mwrite_parquet_to_gcs\u001B[0;34m(df, folder_name)\u001B[0m\n\u001B[1;32m     21\u001B[0m gcs_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgs://\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbucket_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_folder_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Write the DataFrame to Parquet in the specified folder\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgcs_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Use 'overwrite' to write or replace existing content\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData written to Parquet in folder: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_folder_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:1721\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m   1719\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n\u001B[1;32m   1720\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n\u001B[0;32m-> 1721\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n","File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n","File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o925.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 123 in stage 52.0 failed 4 times, most recent failure: Lost task 123.3 in stage 52.0 (TID 1386) (cluster-20241210d-highperformance-deletefast-w-2.us-central1-f.c.cis4400-individual-project.internal executor 19): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to gs://goodreads_bucket/cleaned/reviews.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:493)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\nwriting dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\ninto Parquet INT96 files can be dangerous, as the files may be read by Spark 2.x\nor legacy versions of Hive later, which uses a legacy hybrid calendar that\nis different from Spark 3.0+'s Proleptic Gregorian calendar. See more\ndetails in SPARK-31404. You can set \"spark.sql.parquet.int96RebaseModeInWrite\" to \"LEGACY\" to rebase the\ndatetime values w.r.t. the calendar difference during writing, to get maximum\ninteroperability. Or set the config to \"CORRECTED\" to write the datetime\nvalues as it is, if you are sure that the written files will only be read by\nSpark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createTimestampRebaseFuncInWrite$1(DataSourceUtils.scala:232)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:211)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:210)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:476)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1399)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:483)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2457)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:380)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:329)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:377)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:197)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:473)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:473)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:449)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to gs://goodreads_bucket/cleaned/reviews.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:493)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.WRITE_ANCIENT_DATETIME] You may get a different result due to the upgrading to Spark >= 3.0:\nwriting dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\ninto Parquet INT96 files can be dangerous, as the files may be read by Spark 2.x\nor legacy versions of Hive later, which uses a legacy hybrid calendar that\nis different from Spark 3.0+'s Proleptic Gregorian calendar. See more\ndetails in SPARK-31404. You can set \"spark.sql.parquet.int96RebaseModeInWrite\" to \"LEGACY\" to rebase the\ndatetime values w.r.t. the calendar difference during writing, to get maximum\ninteroperability. Or set the config to \"CORRECTED\" to write the datetime\nvalues as it is, if you are sure that the written files will only be read by\nSpark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sparkUpgradeInWritingDatesError(QueryExecutionErrors.scala:759)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:187)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$createTimestampRebaseFuncInWrite$1(DataSourceUtils.scala:232)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:211)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:210)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:483)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:161)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:471)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:151)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:53)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:476)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1399)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:483)\n\t... 17 more\n"]}],"source":["if ENABLE_SAVE:\n","    write_parquet_to_gcs(reviews_df, \"reviews\")"]},{"cell_type":"code","execution_count":43,"id":"b5ac2524","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Folder cleaned/books does not exist. Creating and writing Parquet.\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Data written to Parquet in folder: cleaned/books\n"]}],"source":["if ENABLE_SAVE:\n","    write_parquet_to_gcs(books_df, \"books\")"]},{"cell_type":"code","execution_count":44,"id":"3c5dfb69","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Folder cleaned/interactions does not exist. Creating and writing Parquet.\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Data written to Parquet in folder: cleaned/interactions\n"]}],"source":["if ENABLE_SAVE:\n","    write_parquet_to_gcs(interactions_df, \"interactions\")"]},{"cell_type":"code","execution_count":45,"id":"3cb82462","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Folder cleaned/authors does not exist. Creating and writing Parquet.\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Data written to Parquet in folder: cleaned/authors\n"]}],"source":["if ENABLE_SAVE:\n","    write_parquet_to_gcs(authors_df, \"authors\")"]},{"cell_type":"code","execution_count":46,"id":"da1e1de9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Folder cleaned/genre does not exist. Creating and writing Parquet.\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Data written to Parquet in folder: cleaned/genre\n"]}],"source":["if ENABLE_SAVE:\n","    write_parquet_to_gcs(genre_df, \"genre\")"]},{"cell_type":"code","execution_count":null,"id":"4ec87c52","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}